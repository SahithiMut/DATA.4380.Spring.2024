{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ppzahhfxDv2d",
        "outputId": "dd0db5c5-01e7-446b-850e-66077611063f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# It imports the Pandas library as \"pd\" alias and reads a CSV file located at '/content/drive/My Drive/stock_prices.csv' into a DataFrame named \"stock_prices_df\".\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the file path in my Google Drive\n",
        "file_path = '/content/drive/My Drive/stock_prices.csv'\n",
        "\n",
        "# Read the CSV file into a DataFrame\n",
        "stock_prices_df = pd.read_csv(file_path)"
      ],
      "metadata": {
        "id": "h4-bggWWEM52"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Access the \"Close\" column of the DataFrame\n",
        "close_values = stock_prices_df['Close']\n",
        "\n",
        "# Print the first few values of the \"Close\" column\n",
        "print(close_values.head())\n",
        "\n",
        "\n",
        "print(close_values.describe())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68zWtBU4ECWd",
        "outputId": "11aca8f0-65a3-46fe-b2c5-736555bc2896"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    2742.0\n",
            "1     571.0\n",
            "2    3210.0\n",
            "3    1550.0\n",
            "4    3330.0\n",
            "Name: Close, dtype: float64\n",
            "count    2.324923e+06\n",
            "mean     2.594023e+03\n",
            "std      3.576538e+03\n",
            "min      1.400000e+01\n",
            "25%      1.022000e+03\n",
            "50%      1.811000e+03\n",
            "75%      3.030000e+03\n",
            "max      1.095500e+05\n",
            "Name: Close, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define new class intervals based on quartiles or percentiles of Close prices\n",
        "q1 = stock_prices_df['Close'].quantile(0.25)\n",
        "q2 = stock_prices_df['Close'].quantile(0.50)\n",
        "q3 = stock_prices_df['Close'].quantile(0.75)\n",
        "class_intervals = [(0, q1), (q1, q2), (q2, q3), (q3, stock_prices_df['Close'].max() + 1)]  # Add 1 to include the maximum value\n",
        "\n",
        "# Define function to assign classes based on target value\n",
        "def assign_class(value):\n",
        "    for i, interval in enumerate(class_intervals, start=1):\n",
        "        if interval[0] <= value < interval[1]:  # Adjust the condition to include values in the third quartile\n",
        "            return f'Class {i}'\n",
        "    return 'Outside Range'\n",
        "\n",
        "# Apply function to assign classes to each data point\n",
        "stock_prices_df['Class'] = stock_prices_df['Close'].apply(assign_class)\n",
        "\n",
        "# Display the DataFrame with assigned classes\n",
        "print(stock_prices_df[['RowId', 'Close', 'Class']])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQuCQx4qD604",
        "outputId": "249d3ec8-49d9-4f9a-82e3-120cea42a6cc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 RowId   Close    Class\n",
            "0        20170104_1301  2742.0  Class 3\n",
            "1        20170104_1332   571.0  Class 1\n",
            "2        20170104_1333  3210.0  Class 4\n",
            "3        20170104_1376  1550.0  Class 2\n",
            "4        20170104_1377  3330.0  Class 4\n",
            "...                ...     ...      ...\n",
            "2332526  20211203_9990   528.0  Class 1\n",
            "2332527  20211203_9991   794.0  Class 1\n",
            "2332528  20211203_9993  1645.0  Class 2\n",
            "2332529  20211203_9994  2389.0  Class 3\n",
            "2332530  20211203_9997   696.0  Class 1\n",
            "\n",
            "[2332531 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Size of the dataset\n",
        "dataset_size = stock_prices_df.shape\n",
        "print(\"Dataset Size:\", dataset_size)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0P__vcsMD2Wo",
        "outputId": "f563833e-aa18-48fd-f02c-c3f75b2261fa"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Size: (2332531, 13)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# To make my life easier so I don't have to type out a longer variable name\n",
        "df = stock_prices_df\n"
      ],
      "metadata": {
        "id": "nhV1eAFtNfGG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Convert 'Date' column to datetime format\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n"
      ],
      "metadata": {
        "id": "srrAMrLnNpGj"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Check for duplicate records\n",
        "duplicates = df.duplicated().sum()\n",
        "print(\"Number of Duplicate Records:\", duplicates)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlBv1kmJNtQq",
        "outputId": "65375445-056d-4df2-88f8-2ce52f9368f4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Duplicate Records: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for format inconsistencies in RowId\n",
        "row_id_format_inconsistencies = df[~df['RowId'].str.match(r'\\d{8}_\\d{4}$')]\n",
        "print(\"Format Inconsistencies in RowId:\")\n",
        "print(row_id_format_inconsistencies)\n",
        "\n",
        "# Check for format inconsistencies in SecuritiesCode\n",
        "securities_code_format_inconsistencies = df[~df['SecuritiesCode'].astype(str).str.isdigit()]\n",
        "print(\"\\nFormat Inconsistencies in SecuritiesCode:\")\n",
        "print(securities_code_format_inconsistencies)\n",
        "\n",
        "# Check for unexpected values in Open, High, Low, Close\n",
        "price_columns = ['Open', 'High', 'Low', 'Close']\n",
        "price_format_inconsistencies = df[~df[price_columns].applymap(lambda x: isinstance(x, (int, float)))].dropna()\n",
        "print(\"\\nFormat Inconsistencies in Price Columns:\")\n",
        "print(price_format_inconsistencies)\n",
        "\n",
        "# Check for format inconsistencies in Volume\n",
        "volume_format_inconsistencies = df[~df['Volume'].astype(str).str.isdigit()]\n",
        "print(\"\\nFormat Inconsistencies in Volume:\")\n",
        "print(volume_format_inconsistencies)\n",
        "\n",
        "# Check for unexpected values in AdjustmentFactor\n",
        "adjustment_factor_format_inconsistencies = df[~df['AdjustmentFactor'].apply(lambda x: isinstance(x, (int, float)))]\n",
        "print(\"\\nFormat Inconsistencies in AdjustmentFactor:\")\n",
        "print(adjustment_factor_format_inconsistencies)\n",
        "\n",
        "\n",
        "\n",
        "# Check for unexpected values in Target\n",
        "Target_inconsistencies = df[~df['Target'].isnull()]\n",
        "print(\"\\nFormat Inconsistencies in Target:\")\n",
        "print(Target_inconsistencies)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ik27onHNwCd",
        "outputId": "ddf7ef14-d982-439c-b7b3-933db82aab9b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Format Inconsistencies in RowId:\n",
            "Empty DataFrame\n",
            "Columns: [RowId, Date, SecuritiesCode, Open, High, Low, Close, Volume, AdjustmentFactor, ExpectedDividend, SupervisionFlag, Target, Class]\n",
            "Index: []\n",
            "\n",
            "Format Inconsistencies in SecuritiesCode:\n",
            "Empty DataFrame\n",
            "Columns: [RowId, Date, SecuritiesCode, Open, High, Low, Close, Volume, AdjustmentFactor, ExpectedDividend, SupervisionFlag, Target, Class]\n",
            "Index: []\n",
            "\n",
            "Format Inconsistencies in Price Columns:\n",
            "Empty DataFrame\n",
            "Columns: [RowId, Date, SecuritiesCode, Open, High, Low, Close, Volume, AdjustmentFactor, ExpectedDividend, SupervisionFlag, Target, Class]\n",
            "Index: []\n",
            "\n",
            "Format Inconsistencies in Volume:\n",
            "Empty DataFrame\n",
            "Columns: [RowId, Date, SecuritiesCode, Open, High, Low, Close, Volume, AdjustmentFactor, ExpectedDividend, SupervisionFlag, Target, Class]\n",
            "Index: []\n",
            "\n",
            "Format Inconsistencies in AdjustmentFactor:\n",
            "Empty DataFrame\n",
            "Columns: [RowId, Date, SecuritiesCode, Open, High, Low, Close, Volume, AdjustmentFactor, ExpectedDividend, SupervisionFlag, Target, Class]\n",
            "Index: []\n",
            "\n",
            "Format Inconsistencies in Target:\n",
            "                 RowId       Date  SecuritiesCode    Open    High     Low  \\\n",
            "0        20170104_1301 2017-01-04            1301  2734.0  2755.0  2730.0   \n",
            "1        20170104_1332 2017-01-04            1332   568.0   576.0   563.0   \n",
            "2        20170104_1333 2017-01-04            1333  3150.0  3210.0  3140.0   \n",
            "3        20170104_1376 2017-01-04            1376  1510.0  1550.0  1510.0   \n",
            "4        20170104_1377 2017-01-04            1377  3270.0  3350.0  3270.0   \n",
            "...                ...        ...             ...     ...     ...     ...   \n",
            "2332526  20211203_9990 2021-12-03            9990   514.0   528.0   513.0   \n",
            "2332527  20211203_9991 2021-12-03            9991   782.0   794.0   782.0   \n",
            "2332528  20211203_9993 2021-12-03            9993  1690.0  1690.0  1645.0   \n",
            "2332529  20211203_9994 2021-12-03            9994  2388.0  2396.0  2380.0   \n",
            "2332530  20211203_9997 2021-12-03            9997   690.0   711.0   686.0   \n",
            "\n",
            "          Close   Volume  AdjustmentFactor  ExpectedDividend  SupervisionFlag  \\\n",
            "0        2742.0    31400               1.0               NaN            False   \n",
            "1         571.0  2798500               1.0               NaN            False   \n",
            "2        3210.0   270800               1.0               NaN            False   \n",
            "3        1550.0    11300               1.0               NaN            False   \n",
            "4        3330.0   150800               1.0               NaN            False   \n",
            "...         ...      ...               ...               ...              ...   \n",
            "2332526   528.0    44200               1.0               NaN            False   \n",
            "2332527   794.0    35900               1.0               NaN            False   \n",
            "2332528  1645.0     7200               1.0               NaN            False   \n",
            "2332529  2389.0     6500               1.0               NaN            False   \n",
            "2332530   696.0   381100               1.0               NaN            False   \n",
            "\n",
            "           Target    Class  \n",
            "0        0.000730  Class 3  \n",
            "1        0.012324  Class 1  \n",
            "2        0.006154  Class 4  \n",
            "3        0.011053  Class 2  \n",
            "4        0.003026  Class 4  \n",
            "...           ...      ...  \n",
            "2332526  0.034816  Class 1  \n",
            "2332527  0.025478  Class 1  \n",
            "2332528 -0.004302  Class 2  \n",
            "2332529  0.009098  Class 3  \n",
            "2332530  0.018414  Class 1  \n",
            "\n",
            "[2332293 rows x 13 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Missing Data\n",
        "missing_data = df.isnull().sum()\n",
        "print(\"Missing Data:\\n\", missing_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_MeEVzFOUc3",
        "outputId": "62f69ed3-a1dd-483e-afc1-d2b993936e2d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing Data:\n",
            " RowId                     0\n",
            "Date                      0\n",
            "SecuritiesCode            0\n",
            "Open                   7608\n",
            "High                   7608\n",
            "Low                    7608\n",
            "Close                  7608\n",
            "Volume                    0\n",
            "AdjustmentFactor          0\n",
            "ExpectedDividend    2313666\n",
            "SupervisionFlag           0\n",
            "Target                  238\n",
            "Class                     0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(columns=['ExpectedDividend'], inplace=True)"
      ],
      "metadata": {
        "id": "7oF4J26DObJW"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate median for each column\n",
        "median_open = df['Open'].median()\n",
        "median_high = df['High'].median()\n",
        "median_low = df['Low'].median()\n",
        "median_close = df['Close'].median()\n",
        "median_Target = df['Target'].median()\n",
        "\n",
        "\n",
        "# Fill missing values with median\n",
        "df['Open'].fillna(median_open, inplace=True)\n",
        "df['High'].fillna(median_high, inplace=True)\n",
        "df['Low'].fillna(median_low, inplace=True)\n",
        "df['Close'].fillna(median_close, inplace=True)\n",
        "df['Target'].fillna(median_Target, inplace=True)\n"
      ],
      "metadata": {
        "id": "5rQo9N7LPub-"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Recheck missingness\n",
        "print(\"Missing Data After Imputation:\\n\", df.isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRd0nbfmQDDg",
        "outputId": "83b34d30-573e-4aa2-c503-4f0b9cc8b165"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing Data After Imputation:\n",
            " RowId               0\n",
            "Date                0\n",
            "SecuritiesCode      0\n",
            "Open                0\n",
            "High                0\n",
            "Low                 0\n",
            "Close               0\n",
            "Volume              0\n",
            "AdjustmentFactor    0\n",
            "SupervisionFlag     0\n",
            "Target              0\n",
            "Class               0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to detect outliers based on the IQR method\n",
        "def detect_outliers(column):\n",
        "    Q1 = column.quantile(0.25)\n",
        "    Q3 = column.quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    return (column < lower_bound) | (column > upper_bound)\n",
        "\n",
        "# Apply outlier detection\n",
        "outliers_open = detect_outliers(df['Open']).sum()\n",
        "outliers_high = detect_outliers(df['High']).sum()\n",
        "outliers_low = detect_outliers(df['Low']).sum()\n",
        "outliers_close = detect_outliers(df['Close']).sum()\n",
        "outliers_volume = detect_outliers(df['Volume']).sum()\n",
        "outliers_adjustment_factor = detect_outliers(df['AdjustmentFactor']).sum()\n",
        "\n",
        "\n",
        "outliers_summary = {\n",
        "    'Open Outliers': outliers_open,\n",
        "    'High Outliers': outliers_high,\n",
        "    'Low Outliers': outliers_low,\n",
        "    'Close Outliers': outliers_close,\n",
        "    'Volume Outliers': outliers_volume,\n",
        "    'AdjustmentFactor Outliers': outliers_adjustment_factor\n",
        "\n",
        "}\n",
        "\n",
        "outliers_summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtXu_cpsQGvO",
        "outputId": "4bd5171b-d784-49b9-8066-1362e229e2a7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Open Outliers': 145183,\n",
              " 'High Outliers': 145803,\n",
              " 'Low Outliers': 144944,\n",
              " 'Close Outliers': 145371,\n",
              " 'Volume Outliers': 308239,\n",
              " 'AdjustmentFactor Outliers': 730}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "# Summary statistics for the 'Volume' column\n",
        "volume_stats = df['Volume'].describe()\n",
        "\n",
        "# Summary statistics for the 'AdjustmentFactor' column\n",
        "adjustment_factor_stats = df['AdjustmentFactor'].describe()\n",
        "\n",
        "# Display summary statistics\n",
        "print(\"Summary Statistics for Volume Column:\")\n",
        "print(volume_stats)\n",
        "print(\"\\nSummary Statistics for AdjustmentFactor Column:\")\n",
        "print(adjustment_factor_stats)\n",
        "\n",
        "# Skewness and kurtosis for both columns\n",
        "volume_skewness = df['Volume'].skew()\n",
        "volume_kurtosis = df['Volume'].kurtosis()\n",
        "\n",
        "adjustment_factor_skewness = df['AdjustmentFactor'].skew()\n",
        "adjustment_factor_kurtosis = df['AdjustmentFactor'].kurtosis()\n",
        "\n",
        "print(\"\\nSkewness for Volume Column:\", volume_skewness)\n",
        "print(\"Kurtosis for Volume Column:\", volume_kurtosis)\n",
        "\n",
        "print(\"\\nSkewness for AdjustmentFactor Column:\", adjustment_factor_skewness)\n",
        "print(\"Kurtosis for AdjustmentFactor Column:\", adjustment_factor_kurtosis)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80qLoT3OQMxV",
        "outputId": "7370a4ed-c471-4326-af71-c0fcefa1aaa0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary Statistics for Volume Column:\n",
            "count    2.332531e+06\n",
            "mean     6.919366e+05\n",
            "std      3.911256e+06\n",
            "min      0.000000e+00\n",
            "25%      3.030000e+04\n",
            "50%      1.071000e+05\n",
            "75%      4.021000e+05\n",
            "max      6.436540e+08\n",
            "Name: Volume, dtype: float64\n",
            "\n",
            "Summary Statistics for AdjustmentFactor Column:\n",
            "count    2.332531e+06\n",
            "mean     1.000508e+00\n",
            "std      6.773040e-02\n",
            "min      1.000000e-01\n",
            "25%      1.000000e+00\n",
            "50%      1.000000e+00\n",
            "75%      1.000000e+00\n",
            "max      2.000000e+01\n",
            "Name: AdjustmentFactor, dtype: float64\n",
            "\n",
            "Skewness for Volume Column: 36.042606002811546\n",
            "Kurtosis for Volume Column: 2368.1649879155293\n",
            "\n",
            "Skewness for AdjustmentFactor Column: 122.99565467169053\n",
            "Kurtosis for AdjustmentFactor Column: 17008.27380321588\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing outliers from volume column\n",
        "\n",
        "#Step 1: Calculate IQR\n",
        "Q1 = df['Volume'].quantile(0.25)\n",
        "Q3 = df['Volume'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "# Step 2: Define lower and upper bounds\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "# Step 3: Filter DataFrame to exclude outliers\n",
        "df_filtered = df[(df['Volume'] >= lower_bound) & (df['Volume'] <= upper_bound)]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4SjopnYdQPrx"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary statistics before outlier removal\n",
        "print(\"Summary Statistics for Volume Column (Before):\")\n",
        "print(df['Volume'].describe())\n",
        "\n",
        "# Summary statistics after outlier removal\n",
        "print(\"\\nSummary Statistics for Volume Column (After):\")\n",
        "print(df_filtered['Volume'].describe())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "al35JOzHQTwY",
        "outputId": "6d6dd2f0-661a-4add-eeaa-a043826933bc"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary Statistics for Volume Column (Before):\n",
            "count    2.332531e+06\n",
            "mean     6.919366e+05\n",
            "std      3.911256e+06\n",
            "min      0.000000e+00\n",
            "25%      3.030000e+04\n",
            "50%      1.071000e+05\n",
            "75%      4.021000e+05\n",
            "max      6.436540e+08\n",
            "Name: Volume, dtype: float64\n",
            "\n",
            "Summary Statistics for Volume Column (After):\n",
            "count    2.024292e+06\n",
            "mean     1.686656e+05\n",
            "std      2.117761e+05\n",
            "min      0.000000e+00\n",
            "25%      2.470000e+04\n",
            "50%      7.830000e+04\n",
            "75%      2.249000e+05\n",
            "max      9.598000e+05\n",
            "Name: Volume, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Log transformation for Volume column\n",
        "df['Volume_Log'] = np.log(df['Volume'])\n",
        "\n",
        "# Display the first few rows to show all columns including the log-transformed Volume\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jz3YPKR5QXLW",
        "outputId": "1a0ad449-1e5f-44f3-ba03-b85d20b77b3f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           RowId       Date  SecuritiesCode    Open    High     Low   Close  \\\n",
            "0  20170104_1301 2017-01-04            1301  2734.0  2755.0  2730.0  2742.0   \n",
            "1  20170104_1332 2017-01-04            1332   568.0   576.0   563.0   571.0   \n",
            "2  20170104_1333 2017-01-04            1333  3150.0  3210.0  3140.0  3210.0   \n",
            "3  20170104_1376 2017-01-04            1376  1510.0  1550.0  1510.0  1550.0   \n",
            "4  20170104_1377 2017-01-04            1377  3270.0  3350.0  3270.0  3330.0   \n",
            "\n",
            "    Volume  AdjustmentFactor  SupervisionFlag    Target    Class  Volume_Log  \n",
            "0    31400               1.0            False  0.000730  Class 3   10.354563  \n",
            "1  2798500               1.0            False  0.012324  Class 1   14.844594  \n",
            "2   270800               1.0            False  0.006154  Class 4   12.509136  \n",
            "3    11300               1.0            False  0.011053  Class 2    9.332558  \n",
            "4   150800               1.0            False  0.003026  Class 4   11.923710  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pandas/core/arraylike.py:396: RuntimeWarning: divide by zero encountered in log\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Min-Max Scaling (Normalization)\n",
        "def min_max_scaling(feature):\n",
        "    min_val = feature.min()\n",
        "    max_val = feature.max()\n",
        "    return (feature - min_val) / (max_val - min_val)\n",
        "\n",
        "# Z-score Normalization (Standardization)\n",
        "def z_score_normalization(feature):\n",
        "    mean = feature.mean()\n",
        "    std_dev = feature.std()\n",
        "    return (feature - mean) / std_dev\n",
        "\n",
        "# Apply Min-Max Scaling and Z-score Normalization to the dataset\n",
        "normalized_df = pd.DataFrame()\n",
        "for col in df.columns:\n",
        "    if df[col].dtype in ['int64', 'float64']:\n",
        "        normalized_df[col + '_minmax'] = min_max_scaling(df[col])\n",
        "        normalized_df[col + '_zscore'] = z_score_normalization(df[col])\n",
        "\n",
        "# Display the normalized dataset\n",
        "print(normalized_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5biUg_gVQaEO",
        "outputId": "866816df-ae3d-4dea-f771-17f23cf1f2b1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   SecuritiesCode_minmax  SecuritiesCode_zscore  Open_minmax  Open_zscore  \\\n",
            "0               0.000000              -1.910785     0.024742     0.039769   \n",
            "1               0.003565              -1.897891     0.005039    -0.566676   \n",
            "2               0.003680              -1.897475     0.028526     0.156243   \n",
            "3               0.008625              -1.879589     0.013608    -0.302931   \n",
            "4               0.008740              -1.879173     0.029617     0.189841   \n",
            "\n",
            "   High_minmax  High_zscore  Low_minmax  Low_zscore  Close_minmax  \\\n",
            "0     0.024800     0.036263    0.025348    0.048551      0.024905   \n",
            "1     0.005078    -0.566714    0.005131   -0.565678      0.005085   \n",
            "2     0.028918     0.162171    0.029173    0.164764      0.029178   \n",
            "3     0.013893    -0.297187    0.013966   -0.297254      0.014023   \n",
            "4     0.030185     0.200913    0.030386    0.201612      0.030273   \n",
            "\n",
            "   Close_zscore  Volume_minmax  Volume_zscore  AdjustmentFactor_minmax  \\\n",
            "0      0.042154       0.000049      -0.168881                 0.045226   \n",
            "1     -0.565803       0.004348       0.538590                 0.045226   \n",
            "2      0.173210       0.000421      -0.107673                 0.045226   \n",
            "3     -0.291648       0.000018      -0.174020                 0.045226   \n",
            "4      0.206815       0.000234      -0.138354                 0.045226   \n",
            "\n",
            "   AdjustmentFactor_zscore  Target_minmax  Target_zscore  Volume_Log_minmax  \\\n",
            "0                -0.007495       0.341139       0.012198                NaN   \n",
            "1                -0.007495       0.347966       0.507697                NaN   \n",
            "2                -0.007495       0.344333       0.243991                NaN   \n",
            "3                -0.007495       0.347218       0.453391                NaN   \n",
            "4                -0.007495       0.342490       0.110296                NaN   \n",
            "\n",
            "   Volume_Log_zscore  \n",
            "0                NaN  \n",
            "1                NaN  \n",
            "2                NaN  \n",
            "3                NaN  \n",
            "4                NaN  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot encode categorical features\n",
        "encoded_df = pd.get_dummies(df, columns=['SupervisionFlag'])\n",
        "\n",
        "# Display the encoded DataFrame\n",
        "print(encoded_df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZXNd70aQeaG",
        "outputId": "5cc1f5e3-8a06-4fa7-d75f-552801afd3a4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           RowId       Date  SecuritiesCode    Open    High     Low   Close  \\\n",
            "0  20170104_1301 2017-01-04            1301  2734.0  2755.0  2730.0  2742.0   \n",
            "1  20170104_1332 2017-01-04            1332   568.0   576.0   563.0   571.0   \n",
            "2  20170104_1333 2017-01-04            1333  3150.0  3210.0  3140.0  3210.0   \n",
            "3  20170104_1376 2017-01-04            1376  1510.0  1550.0  1510.0  1550.0   \n",
            "4  20170104_1377 2017-01-04            1377  3270.0  3350.0  3270.0  3330.0   \n",
            "\n",
            "    Volume  AdjustmentFactor    Target    Class  Volume_Log  \\\n",
            "0    31400               1.0  0.000730  Class 3   10.354563   \n",
            "1  2798500               1.0  0.012324  Class 1   14.844594   \n",
            "2   270800               1.0  0.006154  Class 4   12.509136   \n",
            "3    11300               1.0  0.011053  Class 2    9.332558   \n",
            "4   150800               1.0  0.003026  Class 4   11.923710   \n",
            "\n",
            "   SupervisionFlag_False  SupervisionFlag_True  \n",
            "0                   True                 False  \n",
            "1                   True                 False  \n",
            "2                   True                 False  \n",
            "3                   True                 False  \n",
            "4                   True                 False  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the specified columns\n",
        "df = df.drop(columns=['RowId', 'SupervisionFlag'])"
      ],
      "metadata": {
        "id": "wbeKwzTeQh2I"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming df is your DataFrame\n",
        "data_types = df.dtypes\n",
        "\n",
        "# Print data types along with column names\n",
        "print(\"Data Types:\")\n",
        "print(data_types)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7n8G3AQFUVrE",
        "outputId": "0c2200c0-e1bc-4e5f-a6fe-009b64349339"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Types:\n",
            "Date                datetime64[ns]\n",
            "SecuritiesCode               int64\n",
            "Open                       float64\n",
            "High                       float64\n",
            "Low                        float64\n",
            "Close                      float64\n",
            "Volume                       int64\n",
            "AdjustmentFactor           float64\n",
            "Target                     float64\n",
            "Class                       object\n",
            "Volume_Log                 float64\n",
            "dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Initialize LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Encode the 'Class' column and store the encoded values in a new column 'Class_encoded'\n",
        "stock_prices_df['Class_encoded'] = label_encoder.fit_transform(stock_prices_df['Class'])\n",
        "\n",
        "# Display the DataFrame with encoded classes\n",
        "print(stock_prices_df[['RowId', 'Close', 'Class', 'Class_encoded']])\n",
        "\n",
        "# Drop the original 'Class' column\n",
        "stock_prices_df = stock_prices_df.drop(columns=['Class'])\n",
        "\n",
        "# Verify column names after dropping 'Class'\n",
        "print(stock_prices_df.columns)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoj97C1QYxDR",
        "outputId": "b8ced9e7-32a2-4009-da86-fbd52e504836"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 RowId   Close    Class  Class_encoded\n",
            "0        20170104_1301  2742.0  Class 3              2\n",
            "1        20170104_1332   571.0  Class 1              0\n",
            "2        20170104_1333  3210.0  Class 4              3\n",
            "3        20170104_1376  1550.0  Class 2              1\n",
            "4        20170104_1377  3330.0  Class 4              3\n",
            "...                ...     ...      ...            ...\n",
            "2332526  20211203_9990   528.0  Class 1              0\n",
            "2332527  20211203_9991   794.0  Class 1              0\n",
            "2332528  20211203_9993  1645.0  Class 2              1\n",
            "2332529  20211203_9994  2389.0  Class 3              2\n",
            "2332530  20211203_9997   696.0  Class 1              0\n",
            "\n",
            "[2332531 rows x 4 columns]\n",
            "Index(['RowId', 'Date', 'SecuritiesCode', 'Open', 'High', 'Low', 'Close',\n",
            "       'Volume', 'AdjustmentFactor', 'SupervisionFlag', 'Target', 'Volume_Log',\n",
            "       'Class_encoded'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "# Calculate the total number of samples\n",
        "N_total = df.shape[0]\n",
        "\n",
        "# Specify the proportions for training, validation, and testing sets\n",
        "train_ratio = 0.8\n",
        "val_ratio = 0.1  # Splitting the remaining data equally for validation and testing\n",
        "test_ratio = 0.1\n",
        "\n",
        "# Calculate the number of samples for each set\n",
        "N_train = math.floor(train_ratio * N_total)\n",
        "N_val = math.floor(val_ratio * N_total)\n",
        "N_test = N_total - N_train - N_val\n",
        "\n",
        "# Split the dataset into training, validation, and testing sets\n",
        "X_train = df.iloc[:N_train, :-1]  # Assuming the last column is the target column\n",
        "y_train = df.iloc[:N_train, -1]   # Assuming the last column is the target column\n",
        "\n",
        "X_val = df.iloc[N_train:N_train + N_val, :-1]\n",
        "y_val = df.iloc[N_train:N_train + N_val, -1]\n",
        "\n",
        "X_test = df.iloc[N_train + N_val:, :-1]\n",
        "y_test = df.iloc[N_train + N_val:, -1]\n",
        "\n",
        "# Print the number of samples in the validation set\n",
        "print(\"Number of samples in the validation set:\", N_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXpA80Y6QnhA",
        "outputId": "7c87f33f-0243-4d5a-f6fd-7d951cc76299"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples in the validation set: 233253\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "N_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXKg5ycqROll",
        "outputId": "ad2d3751-2f9d-4a58-eeb8-9857305b33cd"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1866024"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=X_train.shape[1], activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "MbN1sYT2RSki"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgpzBrxERVNl",
        "outputId": "a67ab1cf-ee97-4d16-b5b9-b5592e511ba6"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1st Hidden layer\n",
        "X_train.shape[1]*12 + 12"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_OJ5BosRXx5",
        "outputId": "92fab34e-6dd0-4909-a6ae-27737c56f51c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "132"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2nd Hidden layer\n",
        "12 * 8 + 8"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7DYiXW_RX64",
        "outputId": "7e66509b-7c5c-459e-da99-4eb105e7abeb"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "104"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3rd Hidden layer\n",
        "8 * 8 + 8"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EjCOdcH_Rc8U",
        "outputId": "3466406d-ecb5-4dc9-9448-a403a6794452"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "72"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ByabfQEcRfaF",
        "outputId": "e0692292-22b2-486b-908f-2fc9aa0c2ddb"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 12)                132       \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 8)                 104       \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 8)                 72        \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 9         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 317 (1.24 KB)\n",
            "Trainable params: 317 (1.24 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Dense, Input\n",
        "\n",
        "in_x = Input(shape = X_train.shape[1:])\n",
        "x = Dense(12, input_dim=X_train.shape[1], activation='relu')(in_x)\n",
        "x = Dense(8, activation='relu')(x)\n",
        "x = Dense(8, activation='relu')(x)\n",
        "out_x = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "model = Model(in_x,out_x)\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDVdzoCERhfx",
        "outputId": "88e1d2f5-2b5c-4c24-9545-279cfe4819ff"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 10)]              0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 12)                132       \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 8)                 104       \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 8)                 72        \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 1)                 9         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 317 (1.24 KB)\n",
            "Trainable params: 317 (1.24 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.activations import relu\n",
        "import keras\n",
        "\n",
        "in_x = Input(shape = X_train.shape[1:])\n",
        "x = keras.layers.Normalization(axis=-1)(in_x)\n",
        "x = Dense(12, input_dim=X_train.shape[1], activation='relu')(in_x)\n",
        "x = Dense(8)(x)\n",
        "x = relu(x)\n",
        "out_x = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "model = Model(in_x,out_x)\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFFlfa_TRk69",
        "outputId": "33fe208f-c62e-4e89-d6ff-c313b5134a74"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 10)]              0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 12)                132       \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 8)                 104       \n",
            "                                                                 \n",
            " tf.nn.relu (TFOpLambda)     (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 1)                 9         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 245 (980.00 Byte)\n",
            "Trainable params: 245 (980.00 Byte)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "li3vCfUmRokK"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop timestamp columns from X_train and X_Test\n",
        "X_train_numeric = X_train.drop(columns=['timestamp_column1', 'timestamp_column2', ...], errors='ignore')\n",
        "X_test_numeric = X_test.drop(columns=['timestamp_column1', 'timestamp_column2', ...], errors='ignore')\n",
        "\n",
        "# Convert all columns to numeric (excluding the timestamp columns)\n",
        "X_train_numeric = X_train_numeric.apply(pd.to_numeric, errors='ignore')\n",
        "X_test_numeric = X_test_numeric.apply(pd.to_numeric, errors='ignore')\n",
        "\n",
        "# Convert to NumPy array and cast to float\n",
        "X_train_numeric_array = X_train_numeric.to_numpy().astype(\"float\")\n",
        "X_test_numeric_array = X_test_numeric.to_numpy().astype(\"float\")\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train_numeric_array,\n",
        "                    y_train.to_numpy().astype(\"float\"),\n",
        "                    validation_data=(X_test_numeric_array, y_test.to_numpy().astype(\"float\")),\n",
        "                    epochs=100,\n",
        "                    batch_size=10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9Wk9mDsRx7T",
        "outputId": "cdb81487-0496-48dd-f154-5ff1275ee972"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "186603/186603 [==============================] - 481s 3ms/step - loss: nan - accuracy: 0.2546 - val_loss: nan - val_accuracy: 0.2286\n",
            "Epoch 2/100\n",
            "186603/186603 [==============================] - 477s 3ms/step - loss: nan - accuracy: 0.2546 - val_loss: nan - val_accuracy: 0.2286\n",
            "Epoch 3/100\n",
            "186603/186603 [==============================] - 487s 3ms/step - loss: nan - accuracy: 0.2546 - val_loss: nan - val_accuracy: 0.2286\n",
            "Epoch 4/100\n",
            "186603/186603 [==============================] - 501s 3ms/step - loss: nan - accuracy: 0.2546 - val_loss: nan - val_accuracy: 0.2286\n",
            "Epoch 5/100\n",
            "186603/186603 [==============================] - 482s 3ms/step - loss: nan - accuracy: 0.2546 - val_loss: nan - val_accuracy: 0.2286\n",
            "Epoch 6/100\n",
            "186603/186603 [==============================] - 491s 3ms/step - loss: nan - accuracy: 0.2546 - val_loss: nan - val_accuracy: 0.2286\n",
            "Epoch 7/100\n",
            "186603/186603 [==============================] - 506s 3ms/step - loss: nan - accuracy: 0.2546 - val_loss: nan - val_accuracy: 0.2286\n",
            "Epoch 8/100\n",
            "186603/186603 [==============================] - 474s 3ms/step - loss: nan - accuracy: 0.2546 - val_loss: nan - val_accuracy: 0.2286\n",
            "Epoch 9/100\n",
            "186603/186603 [==============================] - 489s 3ms/step - loss: nan - accuracy: 0.2546 - val_loss: nan - val_accuracy: 0.2286\n",
            "Epoch 10/100\n",
            "186603/186603 [==============================] - 479s 3ms/step - loss: nan - accuracy: 0.2546 - val_loss: nan - val_accuracy: 0.2286\n",
            "Epoch 11/100\n",
            "186603/186603 [==============================] - 466s 2ms/step - loss: nan - accuracy: 0.2546 - val_loss: nan - val_accuracy: 0.2286\n",
            "Epoch 12/100\n",
            "186603/186603 [==============================] - 475s 3ms/step - loss: nan - accuracy: 0.2546 - val_loss: nan - val_accuracy: 0.2286\n",
            "Epoch 13/100\n",
            "186603/186603 [==============================] - 517s 3ms/step - loss: nan - accuracy: 0.2546 - val_loss: nan - val_accuracy: 0.2286\n",
            "Epoch 14/100\n",
            "186603/186603 [==============================] - 503s 3ms/step - loss: nan - accuracy: 0.2546 - val_loss: nan - val_accuracy: 0.2286\n",
            "Epoch 15/100\n",
            "186603/186603 [==============================] - 494s 3ms/step - loss: nan - accuracy: 0.2546 - val_loss: nan - val_accuracy: 0.2286\n",
            "Epoch 16/100\n",
            "186603/186603 [==============================] - 509s 3ms/step - loss: nan - accuracy: 0.2546 - val_loss: nan - val_accuracy: 0.2286\n",
            "Epoch 17/100\n",
            "186603/186603 [==============================] - 507s 3ms/step - loss: nan - accuracy: 0.2546 - val_loss: nan - val_accuracy: 0.2286\n",
            "Epoch 18/100\n",
            "186603/186603 [==============================] - 513s 3ms/step - loss: nan - accuracy: 0.2546 - val_loss: nan - val_accuracy: 0.2286\n",
            "Epoch 19/100\n",
            "186603/186603 [==============================] - 499s 3ms/step - loss: nan - accuracy: 0.2546 - val_loss: nan - val_accuracy: 0.2286\n",
            "Epoch 20/100\n",
            "186603/186603 [==============================] - 524s 3ms/step - loss: nan - accuracy: 0.2546 - val_loss: nan - val_accuracy: 0.2286\n",
            "Epoch 21/100\n",
            "186603/186603 [==============================] - 533s 3ms/step - loss: nan - accuracy: 0.2546 - val_loss: nan - val_accuracy: 0.2286\n",
            "Epoch 22/100\n",
            "186603/186603 [==============================] - 534s 3ms/step - loss: nan - accuracy: 0.2546 - val_loss: nan - val_accuracy: 0.2286\n",
            "Epoch 23/100\n",
            "186603/186603 [==============================] - 536s 3ms/step - loss: nan - accuracy: 0.2546 - val_loss: nan - val_accuracy: 0.2286\n",
            "Epoch 24/100\n",
            "186603/186603 [==============================] - 535s 3ms/step - loss: nan - accuracy: 0.2546 - val_loss: nan - val_accuracy: 0.2286\n",
            "Epoch 25/100\n",
            "186603/186603 [==============================] - 510s 3ms/step - loss: nan - accuracy: 0.2546 - val_loss: nan - val_accuracy: 0.2286\n",
            "Epoch 26/100\n",
            "186603/186603 [==============================] - 517s 3ms/step - loss: nan - accuracy: 0.2546 - val_loss: nan - val_accuracy: 0.2286\n",
            "Epoch 27/100\n",
            "186603/186603 [==============================] - 511s 3ms/step - loss: nan - accuracy: 0.2546 - val_loss: nan - val_accuracy: 0.2286\n",
            "Epoch 28/100\n",
            "186603/186603 [==============================] - 504s 3ms/step - loss: nan - accuracy: 0.2546 - val_loss: nan - val_accuracy: 0.2286\n",
            "Epoch 29/100\n",
            "186603/186603 [==============================] - 509s 3ms/step - loss: nan - accuracy: 0.2546 - val_loss: nan - val_accuracy: 0.2286\n",
            "Epoch 30/100\n",
            "186603/186603 [==============================] - 506s 3ms/step - loss: nan - accuracy: 0.2546 - val_loss: nan - val_accuracy: 0.2286\n",
            "Epoch 31/100\n",
            "186603/186603 [==============================] - 497s 3ms/step - loss: nan - accuracy: 0.2546 - val_loss: nan - val_accuracy: 0.2286\n",
            "Epoch 32/100\n",
            "186603/186603 [==============================] - 501s 3ms/step - loss: nan - accuracy: 0.2546 - val_loss: nan - val_accuracy: 0.2286\n",
            "Epoch 33/100\n",
            "186603/186603 [==============================] - 493s 3ms/step - loss: nan - accuracy: 0.2546 - val_loss: nan - val_accuracy: 0.2286\n",
            "Epoch 34/100\n",
            "186603/186603 [==============================] - 510s 3ms/step - loss: nan - accuracy: 0.2546 - val_loss: nan - val_accuracy: 0.2286\n",
            "Epoch 35/100\n",
            "186603/186603 [==============================] - 498s 3ms/step - loss: nan - accuracy: 0.2546 - val_loss: nan - val_accuracy: 0.2286\n",
            "Epoch 36/100\n",
            "186603/186603 [==============================] - 502s 3ms/step - loss: nan - accuracy: 0.2546 - val_loss: nan - val_accuracy: 0.2286\n",
            "Epoch 37/100\n",
            "159092/186603 [========================>.....] - ETA: 1:10 - loss: nan - accuracy: 0.2545"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop timestamp columns from X_train and X_Test\n",
        "X_train_numeric = X_train.drop(columns=['timestamp_column1', 'timestamp_column2', ...], errors='ignore')\n",
        "X_test_numeric = X_test.drop(columns=['timestamp_column1', 'timestamp_column2', ...], errors='ignore')\n",
        "\n",
        "# Convert all columns to numeric (excluding the timestamp columns)\n",
        "X_train_numeric = X_train_numeric.apply(pd.to_numeric, errors='ignore')\n",
        "X_test_numeric = X_test_numeric.apply(pd.to_numeric, errors='ignore')\n",
        "\n",
        "# Convert to NumPy array and cast to float\n",
        "X_train_numeric_array = X_train_numeric.to_numpy().astype(\"float\")\n",
        "X_test_numeric_array = X_test_numeric.to_numpy().astype(\"float\")\n",
        "\n",
        "# Continue training the model from epoch 38 to epoch 100\n",
        "history = model.fit(X_train_numeric_array,\n",
        "                    y_train.to_numpy().astype(\"float\"),\n",
        "                    validation_data=(X_test_numeric_array, y_test.to_numpy().astype(\"float\")),\n",
        "                    epochs=100,  # Set to the remaining epochs you want to train\n",
        "                    initial_epoch=36,  # Start from epoch 38\n",
        "                    batch_size=10)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4c9ym34CJ64B",
        "outputId": "0e8124eb-2046-47d9-85ed-f592c518f345"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 37/100\n",
            "186603/186603 [==============================] - 413s 2ms/step - loss: nan - accuracy: 0.2538 - val_loss: nan - val_accuracy: 0.2279\n",
            "Epoch 38/100\n",
            "186603/186603 [==============================] - 413s 2ms/step - loss: nan - accuracy: 0.2538 - val_loss: nan - val_accuracy: 0.2279\n",
            "Epoch 39/100\n",
            "186603/186603 [==============================] - 393s 2ms/step - loss: nan - accuracy: 0.2538 - val_loss: nan - val_accuracy: 0.2279\n",
            "Epoch 40/100\n",
            "186603/186603 [==============================] - 399s 2ms/step - loss: nan - accuracy: 0.2538 - val_loss: nan - val_accuracy: 0.2279\n",
            "Epoch 41/100\n",
            "186603/186603 [==============================] - 398s 2ms/step - loss: nan - accuracy: 0.2538 - val_loss: nan - val_accuracy: 0.2279\n",
            "Epoch 42/100\n",
            "186603/186603 [==============================] - 408s 2ms/step - loss: nan - accuracy: 0.2538 - val_loss: nan - val_accuracy: 0.2279\n",
            "Epoch 43/100\n",
            "186603/186603 [==============================] - 387s 2ms/step - loss: nan - accuracy: 0.2538 - val_loss: nan - val_accuracy: 0.2279\n",
            "Epoch 44/100\n",
            "186603/186603 [==============================] - 389s 2ms/step - loss: nan - accuracy: 0.2538 - val_loss: nan - val_accuracy: 0.2279\n",
            "Epoch 45/100\n",
            "186603/186603 [==============================] - 401s 2ms/step - loss: nan - accuracy: 0.2538 - val_loss: nan - val_accuracy: 0.2279\n",
            "Epoch 46/100\n",
            "186603/186603 [==============================] - 399s 2ms/step - loss: nan - accuracy: 0.2538 - val_loss: nan - val_accuracy: 0.2279\n",
            "Epoch 47/100\n",
            "186603/186603 [==============================] - 380s 2ms/step - loss: nan - accuracy: 0.2538 - val_loss: nan - val_accuracy: 0.2279\n",
            "Epoch 48/100\n",
            "186603/186603 [==============================] - 381s 2ms/step - loss: nan - accuracy: 0.2538 - val_loss: nan - val_accuracy: 0.2279\n",
            "Epoch 49/100\n",
            "186603/186603 [==============================] - 381s 2ms/step - loss: nan - accuracy: 0.2538 - val_loss: nan - val_accuracy: 0.2279\n",
            "Epoch 50/100\n",
            "186603/186603 [==============================] - 391s 2ms/step - loss: nan - accuracy: 0.2538 - val_loss: nan - val_accuracy: 0.2279\n",
            "Epoch 51/100\n",
            "186603/186603 [==============================] - 399s 2ms/step - loss: nan - accuracy: 0.2538 - val_loss: nan - val_accuracy: 0.2279\n",
            "Epoch 52/100\n",
            "186603/186603 [==============================] - 388s 2ms/step - loss: nan - accuracy: 0.2538 - val_loss: nan - val_accuracy: 0.2279\n",
            "Epoch 53/100\n",
            "186603/186603 [==============================] - 398s 2ms/step - loss: nan - accuracy: 0.2538 - val_loss: nan - val_accuracy: 0.2279\n",
            "Epoch 54/100\n",
            "186603/186603 [==============================] - 383s 2ms/step - loss: nan - accuracy: 0.2538 - val_loss: nan - val_accuracy: 0.2279\n",
            "Epoch 55/100\n",
            "186603/186603 [==============================] - 392s 2ms/step - loss: nan - accuracy: 0.2538 - val_loss: nan - val_accuracy: 0.2279\n",
            "Epoch 56/100\n",
            "186603/186603 [==============================] - 428s 2ms/step - loss: nan - accuracy: 0.2538 - val_loss: nan - val_accuracy: 0.2279\n",
            "Epoch 57/100\n",
            "186603/186603 [==============================] - 379s 2ms/step - loss: nan - accuracy: 0.2538 - val_loss: nan - val_accuracy: 0.2279\n",
            "Epoch 58/100\n",
            "186603/186603 [==============================] - 380s 2ms/step - loss: nan - accuracy: 0.2538 - val_loss: nan - val_accuracy: 0.2279\n",
            "Epoch 59/100\n",
            "186603/186603 [==============================] - 382s 2ms/step - loss: nan - accuracy: 0.2538 - val_loss: nan - val_accuracy: 0.2279\n",
            "Epoch 60/100\n",
            "186603/186603 [==============================] - 374s 2ms/step - loss: nan - accuracy: 0.2538 - val_loss: nan - val_accuracy: 0.2279\n",
            "Epoch 61/100\n",
            "186603/186603 [==============================] - 374s 2ms/step - loss: nan - accuracy: 0.2538 - val_loss: nan - val_accuracy: 0.2279\n",
            "Epoch 62/100\n",
            "186603/186603 [==============================] - 389s 2ms/step - loss: nan - accuracy: 0.2538 - val_loss: nan - val_accuracy: 0.2279\n",
            "Epoch 63/100\n",
            "186603/186603 [==============================] - 388s 2ms/step - loss: nan - accuracy: 0.2538 - val_loss: nan - val_accuracy: 0.2279\n",
            "Epoch 64/100\n",
            "186603/186603 [==============================] - 389s 2ms/step - loss: nan - accuracy: 0.2538 - val_loss: nan - val_accuracy: 0.2279\n",
            "Epoch 65/100\n",
            "186603/186603 [==============================] - 389s 2ms/step - loss: nan - accuracy: 0.2538 - val_loss: nan - val_accuracy: 0.2279\n",
            "Epoch 66/100\n",
            "186603/186603 [==============================] - 386s 2ms/step - loss: nan - accuracy: 0.2538 - val_loss: nan - val_accuracy: 0.2279\n",
            "Epoch 67/100\n",
            "186603/186603 [==============================] - 385s 2ms/step - loss: nan - accuracy: 0.2538 - val_loss: nan - val_accuracy: 0.2279\n",
            "Epoch 68/100\n",
            "186603/186603 [==============================] - 385s 2ms/step - loss: nan - accuracy: 0.2538 - val_loss: nan - val_accuracy: 0.2279\n",
            "Epoch 69/100\n",
            "186603/186603 [==============================] - 370s 2ms/step - loss: nan - accuracy: 0.2538 - val_loss: nan - val_accuracy: 0.2279\n",
            "Epoch 70/100\n",
            "186603/186603 [==============================] - 388s 2ms/step - loss: nan - accuracy: 0.2538 - val_loss: nan - val_accuracy: 0.2279\n",
            "Epoch 71/100\n",
            "186603/186603 [==============================] - 375s 2ms/step - loss: nan - accuracy: 0.2538 - val_loss: nan - val_accuracy: 0.2279\n",
            "Epoch 72/100\n",
            "186603/186603 [==============================] - 390s 2ms/step - loss: nan - accuracy: 0.2538 - val_loss: nan - val_accuracy: 0.2279\n",
            "Epoch 73/100\n",
            "186603/186603 [==============================] - 381s 2ms/step - loss: nan - accuracy: 0.2538 - val_loss: nan - val_accuracy: 0.2279\n",
            "Epoch 74/100\n",
            "186603/186603 [==============================] - 386s 2ms/step - loss: nan - accuracy: 0.2538 - val_loss: nan - val_accuracy: 0.2279\n",
            "Epoch 75/100\n",
            "186603/186603 [==============================] - 390s 2ms/step - loss: nan - accuracy: 0.2538 - val_loss: nan - val_accuracy: 0.2279\n",
            "Epoch 76/100\n",
            "186603/186603 [==============================] - 377s 2ms/step - loss: nan - accuracy: 0.2538 - val_loss: nan - val_accuracy: 0.2279\n",
            "Epoch 77/100\n",
            "186603/186603 [==============================] - 370s 2ms/step - loss: nan - accuracy: 0.2538 - val_loss: nan - val_accuracy: 0.2279\n",
            "Epoch 78/100\n",
            "186603/186603 [==============================] - 373s 2ms/step - loss: nan - accuracy: 0.2538 - val_loss: nan - val_accuracy: 0.2279\n",
            "Epoch 79/100\n",
            "186603/186603 [==============================] - 384s 2ms/step - loss: nan - accuracy: 0.2538 - val_loss: nan - val_accuracy: 0.2279\n",
            "Epoch 80/100\n",
            "186603/186603 [==============================] - 384s 2ms/step - loss: nan - accuracy: 0.2538 - val_loss: nan - val_accuracy: 0.2279\n",
            "Epoch 81/100\n",
            "186603/186603 [==============================] - 370s 2ms/step - loss: nan - accuracy: 0.2538 - val_loss: nan - val_accuracy: 0.2279\n",
            "Epoch 82/100\n",
            "186603/186603 [==============================] - 384s 2ms/step - loss: nan - accuracy: 0.2538 - val_loss: nan - val_accuracy: 0.2279\n",
            "Epoch 83/100\n",
            "186603/186603 [==============================] - 379s 2ms/step - loss: nan - accuracy: 0.2538 - val_loss: nan - val_accuracy: 0.2279\n",
            "Epoch 84/100\n",
            "186603/186603 [==============================] - 375s 2ms/step - loss: nan - accuracy: 0.2538 - val_loss: nan - val_accuracy: 0.2279\n",
            "Epoch 85/100\n",
            "186603/186603 [==============================] - 387s 2ms/step - loss: nan - accuracy: 0.2538 - val_loss: nan - val_accuracy: 0.2279\n",
            "Epoch 86/100\n",
            "186603/186603 [==============================] - 379s 2ms/step - loss: nan - accuracy: 0.2538 - val_loss: nan - val_accuracy: 0.2279\n",
            "Epoch 87/100\n",
            "186603/186603 [==============================] - 380s 2ms/step - loss: nan - accuracy: 0.2538 - val_loss: nan - val_accuracy: 0.2279\n",
            "Epoch 88/100\n",
            "186603/186603 [==============================] - 381s 2ms/step - loss: nan - accuracy: 0.2538 - val_loss: nan - val_accuracy: 0.2279\n",
            "Epoch 89/100\n",
            "186603/186603 [==============================] - 395s 2ms/step - loss: nan - accuracy: 0.2538 - val_loss: nan - val_accuracy: 0.2279\n",
            "Epoch 90/100\n",
            "186603/186603 [==============================] - 404s 2ms/step - loss: nan - accuracy: 0.2538 - val_loss: nan - val_accuracy: 0.2279\n",
            "Epoch 91/100\n",
            "186603/186603 [==============================] - 383s 2ms/step - loss: nan - accuracy: 0.2538 - val_loss: nan - val_accuracy: 0.2279\n",
            "Epoch 92/100\n",
            "186603/186603 [==============================] - 407s 2ms/step - loss: nan - accuracy: 0.2538 - val_loss: nan - val_accuracy: 0.2279\n",
            "Epoch 93/100\n",
            "186603/186603 [==============================] - 402s 2ms/step - loss: nan - accuracy: 0.2538 - val_loss: nan - val_accuracy: 0.2279\n",
            "Epoch 94/100\n",
            "186603/186603 [==============================] - 393s 2ms/step - loss: nan - accuracy: 0.2538 - val_loss: nan - val_accuracy: 0.2279\n",
            "Epoch 95/100\n",
            "186603/186603 [==============================] - 380s 2ms/step - loss: nan - accuracy: 0.2538 - val_loss: nan - val_accuracy: 0.2279\n",
            "Epoch 96/100\n",
            "186603/186603 [==============================] - 399s 2ms/step - loss: nan - accuracy: 0.2538 - val_loss: nan - val_accuracy: 0.2279\n",
            "Epoch 97/100\n",
            "186594/186603 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.2538"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop timestamp columns from X_train and X_Test\n",
        "X_train_numeric = X_train.drop(columns=['timestamp_column1', 'timestamp_column2', ...], errors='ignore')\n",
        "X_test_numeric = X_test.drop(columns=['timestamp_column1', 'timestamp_column2', ...], errors='ignore')\n",
        "\n",
        "# Convert all columns to numeric (excluding the timestamp columns)\n",
        "X_train_numeric = X_train_numeric.apply(pd.to_numeric, errors='ignore')\n",
        "X_test_numeric = X_test_numeric.apply(pd.to_numeric, errors='ignore')\n",
        "\n",
        "# Convert to NumPy array and cast to float\n",
        "X_train_numeric_array = X_train_numeric.to_numpy().astype(\"float\")\n",
        "X_test_numeric_array = X_test_numeric.to_numpy().astype(\"float\")\n",
        "\n",
        "# Continue training the model from epoch 38 to epoch 100\n",
        "history = model.fit(X_train_numeric_array,\n",
        "                    y_train.to_numpy().astype(\"float\"),\n",
        "                    validation_data=(X_test_numeric_array, y_test.to_numpy().astype(\"float\")),\n",
        "                    epochs=100,  # Set to the remaining epochs you want to train\n",
        "                    initial_epoch=83,\n",
        "                    batch_size=10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gMSzcioWPB0",
        "outputId": "342b9c50-403a-4c82-fa71-ebe7d2c39d14"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 84/100\n",
            "186603/186603 [==============================] - 412s 2ms/step - loss: nan - accuracy: 0.2538 - val_loss: nan - val_accuracy: 0.2279\n",
            "Epoch 85/100\n",
            "186603/186603 [==============================] - 409s 2ms/step - loss: nan - accuracy: 0.2538 - val_loss: nan - val_accuracy: 0.2279\n",
            "Epoch 86/100\n",
            "186603/186603 [==============================] - 392s 2ms/step - loss: nan - accuracy: 0.2538 - val_loss: nan - val_accuracy: 0.2279\n",
            "Epoch 87/100\n",
            "186603/186603 [==============================] - 399s 2ms/step - loss: nan - accuracy: 0.2538 - val_loss: nan - val_accuracy: 0.2279\n",
            "Epoch 88/100\n",
            "186603/186603 [==============================] - 403s 2ms/step - loss: nan - accuracy: 0.2538 - val_loss: nan - val_accuracy: 0.2279\n",
            "Epoch 89/100\n",
            "186603/186603 [==============================] - 402s 2ms/step - loss: nan - accuracy: 0.2538 - val_loss: nan - val_accuracy: 0.2279\n",
            "Epoch 90/100\n",
            "186603/186603 [==============================] - 403s 2ms/step - loss: nan - accuracy: 0.2538 - val_loss: nan - val_accuracy: 0.2279\n",
            "Epoch 91/100\n",
            "186603/186603 [==============================] - 384s 2ms/step - loss: nan - accuracy: 0.2538 - val_loss: nan - val_accuracy: 0.2279\n",
            "Epoch 92/100\n",
            "186603/186603 [==============================] - 389s 2ms/step - loss: nan - accuracy: 0.2538 - val_loss: nan - val_accuracy: 0.2279\n",
            "Epoch 93/100\n",
            "186603/186603 [==============================] - 389s 2ms/step - loss: nan - accuracy: 0.2538 - val_loss: nan - val_accuracy: 0.2279\n",
            "Epoch 94/100\n",
            "186603/186603 [==============================] - 391s 2ms/step - loss: nan - accuracy: 0.2538 - val_loss: nan - val_accuracy: 0.2279\n",
            "Epoch 95/100\n",
            "186603/186603 [==============================] - 388s 2ms/step - loss: nan - accuracy: 0.2538 - val_loss: nan - val_accuracy: 0.2279\n",
            "Epoch 96/100\n",
            "186603/186603 [==============================] - 389s 2ms/step - loss: nan - accuracy: 0.2538 - val_loss: nan - val_accuracy: 0.2279\n",
            "Epoch 97/100\n",
            "186603/186603 [==============================] - 386s 2ms/step - loss: nan - accuracy: 0.2538 - val_loss: nan - val_accuracy: 0.2279\n",
            "Epoch 98/100\n",
            "186603/186603 [==============================] - 398s 2ms/step - loss: nan - accuracy: 0.2538 - val_loss: nan - val_accuracy: 0.2279\n",
            "Epoch 99/100\n",
            "186603/186603 [==============================] - 387s 2ms/step - loss: nan - accuracy: 0.2538 - val_loss: nan - val_accuracy: 0.2279\n",
            "Epoch 100/100\n",
            "186603/186603 [==============================] - 387s 2ms/step - loss: nan - accuracy: 0.2538 - val_loss: nan - val_accuracy: 0.2279\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Initialize LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Encode the 'Class' column\n",
        "df['Class_encoded'] = label_encoder.fit_transform(df['Class'])\n"
      ],
      "metadata": {
        "id": "YioJtERhSMde"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the 'Class' column\n",
        "df = df.drop(columns=['Class'])\n"
      ],
      "metadata": {
        "id": "dzor6kApSP9V"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparison with my previous model"
      ],
      "metadata": {
        "id": "IwJIWPUYJpNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the Node class for the Decision Tree\n",
        "class Node:\n",
        "    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):\n",
        "        self.feature = feature  # Feature index for splitting\n",
        "        self.threshold = threshold  # Threshold for splitting\n",
        "        self.left = left  # Left child node\n",
        "        self.right = right  # Right child node\n",
        "        self.value = value  # Value for leaf nodes\n",
        "\n",
        "# Define the Decision Tree class\n",
        "class DecisionTree:\n",
        "    def __init__(self, max_depth=None, min_samples_split=2):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.root = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.root = self._grow_tree(X, y.astype(int))\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.array([self._predict_row(x, self.root) for x in X])\n",
        "\n",
        "    def _grow_tree(self, X, y, depth=0):\n",
        "        n_samples, n_features = X.shape\n",
        "        n_classes = len(np.unique(y))\n",
        "\n",
        "        # Stopping criteria\n",
        "        if (self.max_depth is not None and depth >= self.max_depth) or n_classes == 1 or n_samples < self.min_samples_split:\n",
        "            value = np.bincount(y).argmax()\n",
        "            return Node(value=value)\n",
        "\n",
        "        # Find the best split\n",
        "        best_gini = float('inf')\n",
        "        best_feature, best_threshold = None, None\n",
        "        for feature in range(n_features):\n",
        "            thresholds = np.unique(X[:, feature])\n",
        "            for threshold in thresholds:\n",
        "                left_indices = np.where(X[:, feature] <= threshold)[0]\n",
        "                right_indices = np.where(X[:, feature] > threshold)[0]\n",
        "\n",
        "                if len(left_indices) == 0 or len(right_indices) == 0:\n",
        "                    continue\n",
        "\n",
        "                gini = self._gini_impurity(y[left_indices], y[right_indices])\n",
        "                if gini < best_gini:\n",
        "                    best_gini = gini\n",
        "                    best_feature = feature\n",
        "                    best_threshold = threshold\n",
        "\n",
        "        # Split the dataset\n",
        "        left_indices = np.where(X[:, best_feature] <= best_threshold)[0]\n",
        "        right_indices = np.where(X[:, best_feature] > best_threshold)[0]\n",
        "        left_child = self._grow_tree(X[left_indices], y[left_indices], depth + 1)\n",
        "        right_child = self._grow_tree(X[right_indices], y[right_indices], depth + 1)\n",
        "\n",
        "        return Node(feature=best_feature, threshold=best_threshold, left=left_child, right=right_child)\n",
        "\n",
        "    def _predict_row(self, x, node):\n",
        "        if node.value is not None:\n",
        "            return node.value\n",
        "        if x[node.feature] <= node.threshold:\n",
        "            return self._predict_row(x, node.left)\n",
        "        else:\n",
        "            return self._predict_row(x, node.right)\n",
        "\n",
        "    def _gini_impurity(self, left_y, right_y):\n",
        "        n = len(left_y) + len(right_y)\n",
        "        p_left = len(left_y) / n\n",
        "        p_right = len(right_y) / n\n",
        "        return p_left * self._calc_gini(left_y) + p_right * self._calc_gini(right_y)\n",
        "\n",
        "    def _calc_gini(self, y):\n",
        "        if len(y) == 0:\n",
        "            return 0\n",
        "        p = np.bincount(np.round(y).astype(int)) / len(y)\n",
        "        return 1 - np.sum(p ** 2)\n",
        "\n",
        "\n",
        "# Define the size of the test set (e.g., 20% of the total data)\n",
        "test_size = 0.2\n",
        "\n",
        "# Split the data into training and test sets\n",
        "train_data = df.sample(frac=1-test_size, random_state=42)  # Use 80% of the data for training\n",
        "test_data = df.drop(train_data.index)  # Use the remaining 20% for testing\n",
        "\n",
        "# Example usage:\n",
        "# Instantiate and train the model\n",
        "tree = DecisionTree(max_depth=3)\n",
        "X_train = train_data.drop(columns=['Close']).values\n",
        "y_train = train_data['Close'].values\n",
        "tree.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "X_test = test_data.drop(columns=['Close']).values\n",
        "predictions = tree.predict(X_test)\n",
        "print(predictions)"
      ],
      "metadata": {
        "id": "Pg46SGnhI9Df"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SgNoIDP5Jn6v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}